import os
from pathlib import Path
from typing import Dict, List, Tuple

import datasets

from seacrowd.utils import schemas
from seacrowd.utils.configs import SEACrowdConfig
from seacrowd.utils.constants import Licenses, Tasks

# TODO: Add BibTeX citation
_CITATION = """\
@Article{app12084083,
AUTHOR = {Onuean, Athita and Buatoom, Uraiwan and Charoenporn, Thatsanee and Kim, Taehong and Jung, Hanmin},
TITLE = {Burapha-TH: A Multi-Purpose Character, Digit, and Syllable Handwriting Dataset},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {8},
ARTICLE-NUMBER = {4083},
URL = {https://www.mdpi.com/2076-3417/12/8/4083},
ISSN = {2076-3417},
DOI = {10.3390/app12084083}
}
"""

# TODO: create a module level variable with your dataset name (should match script name)
#  E.g. Hallmarks of Cancer: [dataset_name] --> hallmarks_of_cancer
_DATASETNAME = "burapha_th"

# TODO: Add description of the dataset here
# You can copy an official description
_DESCRIPTION = """\
The dataset has 68 character classes, 10 digit classes, and 320 syllable classes.
For constructing the dataset, 1072 Thai native speakers wrote on collection datasheets that
were then digitized using a 300 dpi scanner. De-skewing, detection box and segmentation algorithms
were applied to the raw scans for image extraction. The dataset, unlike all other known Thai
handwriting datasets, retains existing noise, the white background, and all artifacts
generated by scanning.
"""

# TODO: Add a link to an official homepage for the dataset here (if possible)
_HOMEPAGE = "https://services.informatics.buu.ac.th/datasets/Burapha-TH/"

# TODO: Add the licence for the dataset here
# Note that this doesn't have to be a common open source license.
# In the case of the dataset intentionally is built without license, please use `Licenses.UNLICENSE.value`
# In the case that it's not clear whether the dataset has a license or not, please use `Licenses.UNKNOWN.value`
# Some datasets may also have custom licenses. In this case, simply put f'{Licenses.OTHERS.value} | {FULL_LICENSE_TERM}' into `_LICENSE`
_LICENSE = Licenses.UNKNOWN.value

_LOCAL = False

_URLS = {
    "character": {"test": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/character/20210306-test.zip", "train": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/character/20210306-train.zip"},
    "digit": {"test": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/digit/20210307-test.zip", "train": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/digit/20210307-train.zip"},
    "syllable": {"test": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/syllable/20210309-test-ori.zip", "train": "https://services.informatics.buu.ac.th/datasets/Burapha-TH/syllable/20210309-train-ori.zip"},
}


# TODO: add supported task by dataset. One dataset may support multiple tasks
_SUPPORTED_TASKS = [Tasks.IMAGE_CAPTIONING]  # example: [Tasks.TRANSLATION, Tasks.NAMED_ENTITY_RECOGNITION, Tasks.RELATION_EXTRACTION]

# TODO: set this to a version that is associated with the dataset. if none exists use "1.0.0"
#  This version doesn't have to be consistent with semantic versioning. Anything that is
#  provided by the original dataset as a version goes.
_SOURCE_VERSION = "1.0.0"

_SEACROWD_VERSION = "1.0.0"

_SUBSETS = ["character", "digit", "syllable"]


def config_constructor(subset: str, schema: str, version: str) -> SEACrowdConfig:
    return SEACrowdConfig(
        name=f"{_DATASETNAME}_{subset}_{schema}",
        version=version,
        description=f"{_DATASETNAME} {subset} {schema} schema",
        schema=f"{schema}",
        subset_id=f"{_DATASETNAME}_{subset}",
    )


class BuraphaThDataset(datasets.GeneratorBasedBuilder):
    """TODO: Short description of my dataset."""

    SOURCE_VERSION = datasets.Version(_SOURCE_VERSION)
    SEACROWD_VERSION = datasets.Version(_SEACROWD_VERSION)

    BUILDER_CONFIGS = [config_constructor(subset, "source", _SOURCE_VERSION) for subset in _SUBSETS]
    BUILDER_CONFIGS.extend([config_constructor(subset, "seacrowd_imtext", _SEACROWD_VERSION) for subset in _SUBSETS])

    DEFAULT_CONFIG_NAME = f"{_DATASETNAME}_digit_source"

    label_chr = [
        "KHO RAKHANG",
        "TO PATAK",
        "THO THONG",
        "SARA UU",
        "SO SALA",
        "DO CHADA",
        "SARA AM",
        "SO SO",
        "FO FAN",
        "CHO CHING",
        "DO DEK",
        "RO RUA",
        "TO TAO",
        "SARA I",
        "MO MA",
        "THO THAN",
        "KHO KHAI",
        "CHO CHANG",
        "THO THAHAN",
        "KHO KHWAI",
        "SARA AI MAIMALAI",
        "THO NANGMONTHO",
        "SO RUSI",
        "KO KAI",
        "SARA E",
        "SARA AA",
        "HO NOKHUK",
        "SARA UE",
        "MAITAIKHU",
        "LU",
        "NO NU",
        "MAI TRI",
        "MAI THO",
        "THANTHAKHAT",
        "YO YAK",
        "KHO KHUAT",
        "CHO CHOE",
        "NGO NGU",
        "SARA UEE",
        "PAIYANNOI",
        "PHO SAMPHAO",
        "PO PLA",
        "YO YING",
        "SARA O",
        "SARA U",
        "LO LING",
        "MAI EK",
        "SO SUA",
        "WO WAEN",
        "PHO PHUNG",
        "LO CHULA",
        "RU",
        "KHO KHON",
        "SARA AI MAIMUAN",
        "SARA II",
        "CHO CHAN",
        "FO FA",
        "O ANG",
        "SARA AE",
        "HO HIP",
        "MAI CHATTAWA",
        "NO NEN",
        "THO THUNG",
        "BO BAIMAI",
        "MAI HAN",
        "SARA A",
        "PHO PHAN",
        "THO PHUTHAO",
    ]
    label_syl = [
        "nak3",
        "waa2",
        "ploi1",
        "nam0",
        "thii2",
        "saao4",
        "sorn4",
        "khun0",
        "phiang0",
        "siang4",
        "eeng0",
        "khian4",
        "noi3",
        "ja1",
        "ork1",
        "mai1",
        "suk1",
        "wat3",
        "koen0",
        "yaang1",
        "pooet1",
        "ton0",
        "ngoen0",
        "dang0",
        "taam0",
        "mai3",
        "kap1",
        "raang2",
        "khor4",
        "baat1",
        "song1",
        "wai3",
        "taa0",
        "la4",
        "dai2",
        "yok3",
        "naaeo0",
        "phaa0",
        "baang0",
        "jut1",
        "aat1",
        "phaet2",
        "thao0",
        "khruu0",
        "khaai4",
        "pii0",
        "phak3",
        "phor2",
        "mueang0",
        "nii3",
        "nuu4",
        "lak1",
        "raan3",
        "morng0",
        "rao0",
        "saam4",
        "jap1",
        "haa4",
        "taai0",
        "dek1",
        "lueak2",
        "jaao2",
        "jat1",
        "thang3",
        "sueng2",
        "baep0",
        "khuan0",
        "khao4",
        "horng2",
        "pai0",
        "fang0",
        "thii0",
        "mak3",
        "khaa2",
        "phaak2",
        "bai0",
        "kha2",
        "rat3",
        "chut3",
        "duu0",
        "maak2",
        "nang2",
        "yorm0",
        "mor4",
        "kaae2",
        "saang2",
        "jop1",
        "rue4",
        "maa0",
        "kham0",
        "ruam2",
        "faai1",
        "chan4",
        "uen1",
        "nii2",
        "thueng4",
        "klaao1",
        "loei0",
        "kwaa1",
        "kheet1",
        "chuea2",
        "thaaen0",
        "thooe0",
        "long0",
        "kaae1",
        "khrang3",
        "yaao0",
        "yang0",
        "khrai0",
        "daan2",
        "hai2",
        "riak2",
        "khong0",
        "khwaam0",
        "mot1",
        "phit1",
        "sip1",
        "khuen2",
        "khaao1",
        "yoo1",
        "torn0",
        "khorng4",
        "yai1",
        "chaat2",
        "phror3",
        "ngai0",
        "plian1",
        "ying2",
        "tor1",
        "koet1",
        "laai4",
        "chue2",
        "phra3",
        "ngaan0",
        "an0",
        "suai4",
        "taang1",
        "dai0",
        "khaao4",
        "lot3",
        "la3",
        "muea2",
        "ao0",
        "ruam0",
        "phaan1",
        "phaap2",
        "nueng1",
        "duean0",
        "naa2",
        "heet1",
        "hua4",
        "tua0",
        "tang2",
        "mai4",
        "klang0",
        "sak1",
        "torp0",
        "naan0",
        "sii4",
        "taae1",
        "jing0",
        "saap2",
        "yaak2",
        "norn0",
        "rot3",
        "truat1",
        "laan3",
        "ngaai2",
        "han4",
        "too0",
        "haeng1",
        "daeng0",
        "lang4",
        "phrorm3",
        "bot1",
        "khrap3",
        "sai1",
        "chen2",
        "khon0",
        "khuen0",
        "khor2",
        "kin0",
        "diao0",
        "yaa0",
        "look2",
        "jit1",
        "klum1",
        "mae2",
        "suung4",
        "are0",
        "thai0",
        "khuu2",
        "chai3",
        "muean4",
        "mi3",
        "choeng0",
        "jueng0",
        "thaam4",
        "plaao1",
        "trong0",
        "paak1",
        "khaao2",
        "dii0",
        "phleeng0",
        "rak3",
        "thue4",
        "lao2",
        "len2",
        "sorng4",
        "doem0",
        "naam3",
        "rian0",
        "chaai0",
        "yaak1",
        "mai2",
        "raeng0",
        "phuu2",
        "sing1",
        "chuai2",
        "klap1",
        "phuea2",
        "khit3",
        "khad0",
        "yuen0",
        "rook2",
        "chan3",
        "hen4",
        "pen0",
        "duai2",
        "laeo3",
        "phuut2",
        "kaae0",
        "naa0",
        "kor2",
        "riip2",
        "suan1",
        "aan1",
        "thuuk1",
        "ton2",
        "ying4",
        "rap3",
        "lae3",
        "raai0",
        "phop3",
        "yuk3",
        "naa2_",
        "tham0",
        "jaak1",
        "plaa0",
        "raek2",
        "rueang2",
        "song0",
        "khrueang2",
        "phoem2",
        "kaan0",
        "klai2",
        "sia4",
        "yorm2",
        "chaao0",
        "naai0",
        "si1",
        "jiin0",
        "ror0",
        "jooe0",
        "bon0",
        "sue3",
        "na3",
        "nai0",
        "phor0",
        "khue0",
        "ruu3",
        "phom4",
        "tit1",
        "phii2",
        "yaa1",
        "baan2",
        "luuk2",
        "jon0",
        "rorp2",
        "khaang2",
        "chuang2",
        "roem2",
        "sat1",
        "wan0",
        "iik1",
        "bork1",
        "nai4",
        "saai4",
        "torng2",
        "thaw1",
        "mue0",
        "thaan2",
        "nork2",
        "khao2",
        "kep1",
        "doi0",
        "thaang0",
        "prap1",
        "waang0",
        "chorp2",
        "mii0",
        "thaa2",
        "luea4",
        "doen0",
        "suu1",
        "norng3",
        "chai2",
        "phuean2",
        "jai0",
        "yim3",
        "man0",
        "nan3",
        "phon4",
        "ruup2",
        "korn1",
        "haak1",
        "phuak2",
        "haai4",
        "lek3",
        "kan0",
    ]
    label_dig = ["NINE", "SIX", "THREE", "TWO", "FIVE", "ZERO", "ONE", "SEVEN", "FOUR", "EIGHT"]

    def _info(self) -> datasets.DatasetInfo:
        task = self.config.subset_id.split("_")[2]
        if self.config.schema == "source":
            features = datasets.Features(
                {"id": datasets.Value("string"), "image_paths": datasets.Value("string"), "label": datasets.Sequence(datasets.ClassLabel(names=self.label_chr if task == "character" else self.label_syl if task == "syllable" else self.label_dig))}
            )
        elif self.config.schema == "seacrowd_imtext":
            features = schemas.image_text_features(label_names=self.label_chr if task == "character" else self.label_syl if task == "syllable" else self.label_dig)
        else:
            raise NotImplementedError()

        return datasets.DatasetInfo(
            description=_DESCRIPTION,
            features=features,
            homepage=_HOMEPAGE,
            license=_LICENSE,
            citation=_CITATION,
        )

    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:
        """Returns SplitGenerators."""

        task = self.config.subset_id.split("_")[2]

        train_path = dl_manager.download_and_extract(_URLS[task]["train"])
        test_path = dl_manager.download_and_extract(_URLS[task]["test"])
        if task != "syllable":
            train_path = os.path.join(train_path, "train")
            test_path = os.path.join(test_path, "test")
        else:
            train_path = os.path.join(train_path, "train-ori")
            test_path = os.path.join(test_path, "test-ori")

        data_pair = {}
        # print([l.split("-")[3].split(" ")[-1] for l in os.listdir(train_path)])
        for dir_name in os.listdir(train_path):
            dir_name_split = dir_name.split("-")
            file_names = []
            for file_name in os.listdir(os.path.join(train_path, dir_name)):
                file_names.append(os.path.join(train_path, dir_name, file_name))
                if task == "character":
                    label = dir_name_split[3]
                    data_pair[label] = file_names
                elif task == "digit":
                    label = dir_name_split[3].split(" ")[-1]
                    data_pair[label] = file_names
                elif task == "syllable":
                    if dir_name_split[0] == "294":
                        dir_name_split[1] = "naa2_"
                    label = f"{dir_name_split[1]} {dir_name_split[2]}"
                    data_pair[label] = file_names

        return [
            datasets.SplitGenerator(
                name=datasets.Split.TRAIN,
                gen_kwargs={
                    "filepath": data_pair,
                    "split": "train",
                },
            ),
            datasets.SplitGenerator(
                name=datasets.Split.TEST,
                gen_kwargs={
                    "filepath": data_pair,
                    "split": "test",
                },
            ),
        ]

    def _generate_examples(self, filepath: Path, split: str) -> Tuple[int, Dict]:
        """Yields examples as (key, example) tuples."""
        task = self.config.subset_id.split("_")[2]
        counter = 0

        for key, imgs in filepath.items():
            for img in imgs:
                if self.config.schema == "source":
                    yield counter, {"id": str(counter), "image_paths": img, "label": [self.label_chr.index(key) if task == "character" else self.label_syl.index(key.split(" ")[0]) if task == "syllable" else self.label_dig.index(key)]}
                elif self.config.schema == "seacrowd_imtext":
                    yield counter, {
                        "id": str(counter),
                        "image_paths": [img],
                        "texts": None,
                        "metadata": {
                            "context": None,
                            "labels": [self.label_chr.index(key) if task == "character" else self.label_syl.index(key.split(" ")[0]) if task == "syllable" else self.label_dig.index(key)],
                        },
                    }
                counter += 1
